# 算法工程师面试-问题汇总
记录一些机器学习、深度学习、NLP、视觉等等算法打工人面试时的常见问题，也是对自己基础的自纠自查。
持续补充，欢迎一起补充成长~

## 数据类问题

简述样本不平衡的定义和解决方法

样本类别不平衡会造成问题的两个主要原因

数据增强都有哪些方法?

如何解决图像细节不足问题(增强特征提取骨干网络的表达能力)

如何解决过拟合问题?

什么是过拟合?产生过拟合的根本原因

什么是欠拟合?产生欠拟合的原因?

标准化与归一化的区别?

如何处理特征向量的缺失值?

数据预处理方法有哪些?

什么是特征归一化?

介绍一下过拟合和欠拟合

如何解决欠拟合?

如何防止过拟合?

为什么要对特征做归一化?理解清楚特征归一化所适用的模型场景

什么是数据扩充并举例说明?



## 正则化

简述正则化的原理

介绍一下LO、L1、L2

正则化L1、L2正则化的区别

实现参数的稀疏有什么优点?

介绍一下L1、L2正则化的作用

正则化有哪几种，分别有什么作用?

在正则化中，L1趋向于0，但L2不会，为什么?

L1和L2正则先验分别服从什么分布?



## 机器学习理论

什么是偏差和方差?

监督学习和非监督学习有什么不同?

训练集、测试集和验证集的作用

模型训练为什么要进行shuffle?

介绍一下生成式模型和判别式模型

有监督学习和无监督学习的区别

线性分类器与非线性分类器的区别以及优劣

简述一个完整机器学习项目的流程

什么是组合特征?如何处理高维组合特征?

为什么一些场景中 使用余弦相似度而不是欧式距离?

hot编码和数字编码的特点

K折交叉验证(k-foldcrossvalidation)具体是怎么做的?

怎么求拐点?

鞍点的定义

假设检验的基本思想

简述熵的定义，写出熵的计算公式

KL散度的计算公式

结构风险和经验风险怎么理解?

分类和回归的区别，各举例3个

模型梯度消失和爆炸产生的原因及解决方法?

判别模型与生成模型的本质区别是什么



## 评价指标

模型评价指标都有什么，AUC是什么?

机器学习泛化能力评测指标

回归问题评价指标

简述AUC，ROC，mAP，Recall，Precision，F1-score

解释一下ROC曲线的原理



## 支持向量机SVM

介绍一下SVM算法

SVM算法的优缺点?

如何理解SVM，对偶问题、核函数都是怎么来的?

SVM的优化函数怎么写，代价函数是什么?

SVM的原理SVM的核函数都有哪些?

为什么要用核函数?简述SVM核函数之间的区别

SVM的核函数如何选择?

SVM如何解决线性不可分问题?

SVM优化的目标是啥?

SVM的损失函数，SVM的适用场景

SVM为什么要引用对偶问题?

牛顿法和拟牛顿法介绍

简述LR和SVM的区别，以及SVM的应用场景简述支持向量回归(SVR)的原理

SVM为什么采用间隔最大化?

为什么SVM对缺失数据敏感?

SVM的超参数C如何调节?

简述SVM硬间隔推导过程

简述SVM软间隔推导过程

支持向量机(SVM)是否适合大规模数据?

LinearSVM和LR有什么异同?



## 逻辑回归LR

介绍一下逻辑斯蒂回归算法?

线性回归和逻辑回归的区别

如何优化LR?

LR与决策树的区别

逻辑回归相比线性回归，有何异同?

推导逻辑回归的损失函数(从极大似然函数出发)

SVM和logistic回归分别在什么情况下使用?

逻辑斯蒂回归能否解决非线性分类问题?

为什么LR可以用来做CTR预估?

naivebayes和logisticregression的区别



## 树模型

有哪些决策树算法?

ID3，C4.5和CART树决策树都用什么指标，信息增益是什么?

对于树形结构为什么不需要归一化?说明特征归一化所适用的模型场景

随机森林的原理

随机森林的随机体现在哪里?

随机森林如何处理缺失值。

随机森林如何评估特征重要性

随机森林和GBDT区别

GBDT为什么拟合负梯度而不是残差

简单介绍GBDT算法的原理

随机森林算法的优缺点?

随机森林和GBDT的区别?

GBDT如何用于分类?

为什么GBDT不适合使用高维稀疏特征?

GBDT算法的优缺点?

简述决策树的构建过程?

决策树的优缺点?

决策树如何防止过拟合?

说说具体方法集成学习集成学习bagging和boosting的联系和区别

bagging和boosting是怎么做的，和他们的比较stacking是什么?需要注意哪些问题?

GBDT和XGBoost的区别?

XGBoost叶子结点的值怎么计算的?

LightGBM对于XGBoost有什么改进?

什么是集成学习算法?

集成学习主要有哪几种框架，并简述它们的工作过程?

Boosting算法有哪两类，它们之间的区别是什么?

One-hot的作用是什么?为什么不直接使用数字作为表示?

选择叶子节点的评价指标都有什么?

对于连续值，怎么选择分割点?

XGBOOST相对于GDBT最大的优势是什么，XGBOOST求二阶导数的好处都有什么?

为什么说Bagging可以减少弱分类器的方差，而Boosting可以减少弱分类器的偏差?

XGBoost为什么可以并行训练?

XGBoost防止过拟合的方法?

XGBoost为什么这么快?

CatBoost有什么特点?



## 贝叶斯

简单说说贝叶斯定理

极大似然估计和最大后验估计的区别是什么?

什么是贝叶斯定理，它是如何使用在机器学习中的?

写出全概率公式&贝叶斯公式

朴素贝叶斯为什么“朴素naive”?

朴素贝叶斯有没有超参数可以调?

根据公式来判断朴素贝叶斯的工作流程是怎样的?

朴素贝叶斯对异常值敏不敏感?

朴素贝叶斯和贝叶斯的关系和区别?

你能给我说说朴素贝叶斯有什么优缺点吗?

什么朴素贝叶斯的预测仍然可以取得较好的效果?

什么是拉普拉斯平滑法?

朴素贝叶斯中有多少种模型?

你知道朴素贝叶斯有哪些应用吗?

朴素贝叶斯是高方差还是低方差模型?



## 聚类算法

聚类算法有哪些?

有哪些聚类算法?

聚类算法中的距离度量有哪些?

说说聚类算法DBSCAN?

KNN和k-means聚类由什么不同?

K-means聚类的原理以及过程?

K-means聚类怎么衡量相似度的?

简要阐述一下KNN算法和K-Means算法的区别

Kmeans和EM算法很相似，类比一下?

讲一下k-means与谱聚类

kMeans初始类簇中心点的选取

简述kmeans流程，思考流程中存在的缺陷和可以改进的策略

Kmeans的复杂度?

PCA是什么?实现过程是什么，意义是什么?

对比降维方法主成分分析(PCA)和线性判别分析(LDA)

PCA属于有监督还是无监督

聊聊最大期望EM算法?

kmeans对异常值是否敏感?思考异常值可能会影响哪个步骤?

如何评估聚类效果?

Kmeans的复杂度?

余弦相似度距离和欧式距离的区别? 

欧式距离与曼哈顿距离的比较？数值特点有哪些。

PCA对比降维方法

PCA属于有监督还是无监督







# 深度学习

## 基础

Normalization相关：BN，Layer Normalization，Instance Normalization，Group Normalization 的作用、实现、相互间区别、使用场景

优化算法介绍：SGD、SGDM、NAG、AdaGrad、RMSprop、Adam、NAdam、AdaMax、AMSGrad

损失函数使用过哪些：MAE, MSE, Smooth L1 Loss, Huber Loss, Log-Cosh, Quantile, Hinge Loss, 交叉熵损失, Focal Loss

分类为什么不用mse而是交叉熵

各种激活函数的特性：Sigmoid，Tanh，ReLU，Leaky ReLU，ELU，GELU

梯度消失与梯度爆炸产生的原因

如何防止模型过拟合或欠拟合

参数初始化的方法有哪些，介绍一下Xavier，kaiming_normal

加入参数初始化时都设置为0会带来什么影响



## NLP

文本表示模型有哪些，优缺点是什么

word2vec和onehot的区别

WORD2VEC介绍，训练和优化方法有哪些

word2vec与LDA的区别于联系

了解过哪些命名实体识别方法，优缺点有哪些

隐马尔可夫模型(HMM)

最大熵隐马尔可夫模型为什么会出现标注偏置问题，如何解决呢？

介绍一下条件随机场(CRF)

CRF与HMM的区别

常见的概率图模型中，哪些是生成式模型，哪些是判别式模型？

RNN 与 LSTM，Bi-LSTM，GRU 原理

rnn为什么梯度消失和梯度爆炸

lstm原理三个门作用

LSTM三个门分别使用什么激活函数，为什么

LSTM为什么能解决梯度消失和梯度爆炸

Memory Network的介绍

seq2seq有哪些解码方法

seq2seq模型为什么需要加入注意力机制

attention有哪些种类

self-attention 原理

transformer 原理与优缺点

transformer为什么固定句子长度，如何处理超长问题

transformer存在哪些问题

BERT 原理

bert 的变种模型

BERT的位置编码如何实现，尝试过哪些其他的位置编码方法

Position encoding是相加还是concat？为什么

Position encoding和 Position embedding的区别？

为什么采用点积模型的 self-attention 而不采用加性模型？

bert的根号dk作用，为什么要除以维度的开根号

self-attention 如何解决长距离依赖问题？

self-attention 如何并行化？

多头注意力原理与作用

描述下多头自注意力机制，自注意力公式

说说bert为什么用LN而不是BN

bert为什么用残差结构

bert中有用到哪些mask方法





## 视觉

CNN算法了解过哪些

卷积神经网络为什么会具有平移不变性？

卷积神经网络中im2col是如何实现的？

池化的作用是什么，有哪些种类

池化层是如何反向传播的

什么是空洞卷积

解释反卷积的原理和用途

faster rcnn的最后输出大小是多少、它的正负样本是怎么选择

目标检测中如何解决目标尺度大小不一的情况

YOLO 系列算法的演进

anchor-free 的目标检测算法

卷积有哪些类型：普通卷积，Group卷积，深度可分离卷积实现与参数量计算、转置卷积、空洞卷积、1*1 卷积（变换channel，通道融合，降维）、deformable conv

ROI pooling 与 ROI align

IOU怎么定义

非极大值抑制（NMS）

目标检测，任意形状的检测框应该怎样实现

Transformer相比较CNN的优缺点

FPN为何能够提升小目标的精度

resnet和densenet及其不同

pooling层的作用



## 强化学习

介绍一下生成对抗网络GAN





## 模型压缩

有哪些模型压缩方法

模型剪枝是怎么实现的

模型蒸馏介绍，损失函数如何定义

参数量化有哪些方法





# python相关

python多线程，多进程

python的GIL

生成器与迭代器

装饰器

深浅拷贝

python 的垃圾回收机制

